{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification - Diabetes Prediction\n\nThis notebook implements binary classification for diabetes prediction with confidence estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\nfrom sklearn.calibration import calibration_curve, CalibratedClassifierCV\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('Libraries loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/diabetes_binary.csv')\nprint(f'Dataset shape: {df.shape}')\nprint(f'Target distribution:\\n{df[\"Outcome\"].value_counts()}')\ndf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Outcome', axis=1)\ny = df['Outcome']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f'Training set: {X_train.shape}')\nprint(f'Test set: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\nmodels = {\n    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n    'SVM': SVC(kernel='rbf', probability=True, random_state=42)\n}\n\nresults = {}\nfor name, model in models.items():\n    print(f'\\nTraining {name}...')\n    model.fit(X_train_scaled, y_train)\n    y_pred = model.predict(X_test_scaled)\n    y_proba = model.predict_proba(X_test_scaled)[:, 1]\n    \n    results[name] = {\n        'model': model,\n        'accuracy': accuracy_score(y_test, y_pred),\n        'precision': precision_score(y_test, y_pred),\n        'recall': recall_score(y_test, y_pred),\n        'f1': f1_score(y_test, y_pred),\n        'roc_auc': roc_auc_score(y_test, y_proba),\n        'y_pred': y_pred,\n        'y_proba': y_proba\n    }\n    \nprint('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\nperf_df = pd.DataFrame({name: {k: v for k, v in data.items() if k not in ['model', 'y_pred', 'y_proba']} for name, data in results.items()}).T\nprint('\\nModel Performance Comparison:')\nprint(perf_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nmetrics = ['accuracy', 'precision', 'recall', 'f1']\nfor idx, metric in enumerate(metrics):\n    ax = axes[idx // 2, idx % 2]\n    values = [results[m][metric] for m in models.keys()]\n    ax.bar(models.keys(), values, color=['#3498db', '#2ecc71', '#f39c12', '#e74c3c'])\n    ax.set_title(f'{metric.capitalize()} Comparison', fontsize=12, fontweight='bold')\n    ax.set_ylabel(metric.capitalize())\n    ax.set_ylim([0, 1])\n    ax.tick_params(axis='x', rotation=45)\n    for i, v in enumerate(values):\n        ax.text(i, v + 0.02, f'{v:.3f}', ha='center')\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\nfor idx, (name, data) in enumerate(results.items()):\n    ax = axes[idx // 2, idx % 2]\n    cm = confusion_matrix(y_test, data['y_pred'])\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n    ax.set_title(f'{name}\\nConfusion Matrix')\n    ax.set_ylabel('True Label')\n    ax.set_xlabel('Predicted Label')\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\nfor name, data in results.items():\n    fpr, tpr, _ = roc_curve(y_test, data['y_proba'])\n    plt.plot(fpr, tpr, label=f'{name} (AUC = {data[\"roc_auc\"]:.3f})', linewidth=2)\nplt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('ROC Curves - Binary Classification', fontsize=14, fontweight='bold')\nplt.legend(loc='lower right')\nplt.grid(alpha=0.3)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Estimation Techniques\n\n### 1. Prediction Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = max(results, key=lambda x: results[x]['roc_auc'])\nbest_model = results[best_model_name]['model']\nprint(f'Best model: {best_model_name}')\n\n# Get prediction probabilities\nproba = best_model.predict_proba(X_test_scaled)\nconfidence = np.max(proba, axis=1)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.hist(confidence, bins=30, edgecolor='black')\nplt.xlabel('Confidence')\nplt.ylabel('Frequency')\nplt.title(f'{best_model_name}\\nPrediction Confidence Distribution')\n\nplt.subplot(1, 2, 2)\nplt.scatter(range(len(confidence)), confidence, c=y_test, cmap='RdYlGn', alpha=0.6)\nplt.xlabel('Sample Index')\nplt.ylabel('Confidence')\nplt.title('Confidence by Sample')\nplt.colorbar(label='True Label')\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\nfor idx, (name, data) in enumerate(results.items()):\n    ax = axes[idx // 2, idx % 2]\n    prob_true, prob_pred = calibration_curve(y_test, data['y_proba'], n_bins=10)\n    ax.plot(prob_pred, prob_true, marker='o', linewidth=2, label=name)\n    ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n    ax.set_xlabel('Predicted Probability')\n    ax.set_ylabel('True Probability')\n    ax.set_title(f'{name}\\nCalibration Curve')\n    ax.legend()\n    ax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calibrated Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply calibration\ncalibrated_model = CalibratedClassifierCV(best_model, method='sigmoid', cv=5)\ncalibrated_model.fit(X_train_scaled, y_train)\n\ny_proba_calibrated = calibrated_model.predict_proba(X_test_scaled)[:, 1]\n\n# Compare calibration\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Before calibration\nprob_true, prob_pred = calibration_curve(y_test, results[best_model_name]['y_proba'], n_bins=10)\naxes[0].plot(prob_pred, prob_true, marker='o', linewidth=2, label='Before Calibration')\naxes[0].plot([0, 1], [0, 1], 'k--', label='Perfect')\naxes[0].set_xlabel('Predicted Probability')\naxes[0].set_ylabel('True Probability')\naxes[0].set_title('Before Calibration')\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\n# After calibration\nprob_true_cal, prob_pred_cal = calibration_curve(y_test, y_proba_calibrated, n_bins=10)\naxes[1].plot(prob_pred_cal, prob_true_cal, marker='o', linewidth=2, label='After Calibration', color='green')\naxes[1].plot([0, 1], [0, 1], 'k--', label='Perfect')\naxes[1].set_xlabel('Predicted Probability')\naxes[1].set_ylabel('True Probability')\naxes[1].set_title('After Calibration')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Bootstrap Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n\nn_iterations = 100\nscores = []\n\nfor _ in range(n_iterations):\n    X_boot, y_boot = resample(X_test_scaled, y_test, random_state=_)\n    y_pred_boot = best_model.predict(X_boot)\n    scores.append(accuracy_score(y_boot, y_pred_boot))\n\nmean_score = np.mean(scores)\nci_lower = np.percentile(scores, 2.5)\nci_upper = np.percentile(scores, 97.5)\n\nprint(f'Bootstrap Accuracy: {mean_score:.4f}')\nprint(f'95% Confidence Interval: [{ci_lower:.4f}, {ci_upper:.4f}]')\n\nplt.figure(figsize=(10, 5))\nplt.hist(scores, bins=20, edgecolor='black', alpha=0.7)\nplt.axvline(mean_score, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_score:.4f}')\nplt.axvline(ci_lower, color='green', linestyle='--', linewidth=2, label=f'CI Lower: {ci_lower:.4f}')\nplt.axvline(ci_upper, color='green', linestyle='--', linewidth=2, label=f'CI Upper: {ci_upper:.4f}')\nplt.xlabel('Accuracy')\nplt.ylabel('Frequency')\nplt.title('Bootstrap Accuracy Distribution')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n\nIn this notebook, we:\n1. Trained multiple classification models\n2. Evaluated performance metrics\n3. Implemented confidence estimation techniques:\n   - Prediction probabilities\n   - Calibration curves\n   - Calibrated predictions\n   - Bootstrap confidence intervals\n\nThe best performing model can now be used for diabetes prediction with confidence estimates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}