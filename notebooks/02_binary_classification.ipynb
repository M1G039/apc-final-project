{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification - Diabetes Prediction\n",
    "\n",
    "This notebook implements binary classification for diabetes prediction with confidence estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('Libraries loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/diabetes_binary.csv')\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'Target distribution:\\n{df[\"Outcome\"].value_counts()}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Outcome', axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f'Training set: {X_train.shape}')\n",
    "print(f'Test set: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f'\\nTraining {name}...')\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_proba),\n",
    "        'y_pred': y_pred,\n",
    "        'y_proba': y_proba\n",
    "    }\n",
    "    \n",
    "print('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "perf_df = pd.DataFrame({name: {k: v for k, v in data.items() if k not in ['model', 'y_pred', 'y_proba']} for name, data in results.items()}).T\n",
    "print('\\nModel Performance Comparison:')\n",
    "print(perf_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    values = [results[m][metric] for m in models.keys()]\n",
    "    ax.bar(models.keys(), values, color=['#3498db', '#2ecc71', '#f39c12', '#e74c3c'])\n",
    "    ax.set_title(f'{metric.capitalize()} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(metric.capitalize())\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    for i, v in enumerate(values):\n",
    "        ax.text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "for idx, (name, data) in enumerate(results.items()):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    cm = confusion_matrix(y_test, data['y_pred'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_title(f'{name}\\nConfusion Matrix')\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "for name, data in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, data['y_proba'])\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {data[\"roc_auc\"]:.3f})', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Binary Classification', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Estimation Techniques\n",
    "\n",
    "### 1. Prediction Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = max(results, key=lambda x: results[x]['roc_auc'])\n",
    "best_model = results[best_model_name]['model']\n",
    "print(f'Best model: {best_model_name}')\n",
    "\n",
    "# Get prediction probabilities\n",
    "proba = best_model.predict_proba(X_test_scaled)\n",
    "confidence = np.max(proba, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(confidence, bins=30, edgecolor='black')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'{best_model_name}\\nPrediction Confidence Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(range(len(confidence)), confidence, c=y_test, cmap='RdYlGn', alpha=0.6)\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Confidence by Sample')\n",
    "plt.colorbar(label='True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "for idx, (name, data) in enumerate(results.items()):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    prob_true, prob_pred = calibration_curve(y_test, data['y_proba'], n_bins=10)\n",
    "    ax.plot(prob_pred, prob_true, marker='o', linewidth=2, label=name)\n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    ax.set_xlabel('Predicted Probability')\n",
    "    ax.set_ylabel('True Probability')\n",
    "    ax.set_title(f'{name}\\nCalibration Curve')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calibrated Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply calibration\n",
    "calibrated_model = CalibratedClassifierCV(best_model, method='sigmoid', cv=5)\n",
    "calibrated_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_proba_calibrated = calibrated_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Compare calibration\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before calibration\n",
    "prob_true, prob_pred = calibration_curve(y_test, results[best_model_name]['y_proba'], n_bins=10)\n",
    "axes[0].plot(prob_pred, prob_true, marker='o', linewidth=2, label='Before Calibration')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Perfect')\n",
    "axes[0].set_xlabel('Predicted Probability')\n",
    "axes[0].set_ylabel('True Probability')\n",
    "axes[0].set_title('Before Calibration')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# After calibration\n",
    "prob_true_cal, prob_pred_cal = calibration_curve(y_test, y_proba_calibrated, n_bins=10)\n",
    "axes[1].plot(prob_pred_cal, prob_true_cal, marker='o', linewidth=2, label='After Calibration', color='green')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Perfect')\n",
    "axes[1].set_xlabel('Predicted Probability')\n",
    "axes[1].set_ylabel('True Probability')\n",
    "axes[1].set_title('After Calibration')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Bootstrap Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "n_iterations = 100\n",
    "scores = []\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    X_boot, y_boot = resample(X_test_scaled, y_test, random_state=_)\n",
    "    y_pred_boot = best_model.predict(X_boot)\n",
    "    scores.append(accuracy_score(y_boot, y_pred_boot))\n",
    "\n",
    "mean_score = np.mean(scores)\n",
    "ci_lower = np.percentile(scores, 2.5)\n",
    "ci_upper = np.percentile(scores, 97.5)\n",
    "\n",
    "print(f'Bootstrap Accuracy: {mean_score:.4f}')\n",
    "print(f'95% Confidence Interval: [{ci_lower:.4f}, {ci_upper:.4f}]')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(scores, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(mean_score, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_score:.4f}')\n",
    "plt.axvline(ci_lower, color='green', linestyle='--', linewidth=2, label=f'CI Lower: {ci_lower:.4f}')\n",
    "plt.axvline(ci_upper, color='green', linestyle='--', linewidth=2, label=f'CI Upper: {ci_upper:.4f}')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bootstrap Accuracy Distribution')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "1. Trained multiple classification models\n",
    "2. Evaluated performance metrics\n",
    "3. Implemented confidence estimation techniques:\n",
    "   - Prediction probabilities\n",
    "   - Calibration curves\n",
    "   - Calibrated predictions\n",
    "   - Bootstrap confidence intervals\n",
    "\n",
    "The best performing model can now be used for diabetes prediction with confidence estimates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
